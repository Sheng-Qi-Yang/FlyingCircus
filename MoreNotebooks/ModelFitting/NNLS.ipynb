{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6-aboTeFyO9l"
   },
   "source": [
    "# An Introduction to Model Fitting with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "asb7v9LvyO9m"
   },
   "source": [
    "In this notebook, I assume you have used python to some degree to analyze data. I will be using numpy/scipy, the de-facto numerical workhorse in python. I will also use matplotlib to visualize the data. We're going to fit a model to some 'fake' data:  a constant continuum with a Gaussian line superimposed. The [sequel to this notebook](Emcee.ipynb) will be model fitting with Markov Chain Monte Carlo techniques (MCMC). But first, let's make the fake data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-the6P_WyO9n"
   },
   "source": [
    "## 1 Making a Fake Emission Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UsoGIVwwyO9o"
   },
   "source": [
    "The \"true\" data is some background flux of photons (a continuum from the source or background) that has a linear trend, plus a Gaussian line with some amplitude, width and center. I set these up as variables so it's easy to play around with them and see how things change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XmZC5gLHyO9p"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Start by defining some parameters. Change these if you like!\n",
    "cont_zp = 500.0   # value at 0\n",
    "cont_slope = 5.0  # change in continuum per channel \n",
    "amplitude = 30.0 # peak of the line\n",
    "width = 0.5       # Width of the line\n",
    "center = 5.0      # location of the line\n",
    "\n",
    "# Next, a grid of wavelength channels (assumed to have no uncertainty)\n",
    "wave = np.linspace(0,10,100)\n",
    "# The 'true' observations\n",
    "flux = amplitude*np.exp(-0.5*np.power(wave-center,2)/width**2)+ \\\n",
    "       cont_zp + + cont_slope*wave\n",
    "# The actual observations = true observations + Poisson noise\n",
    "obs_flux = np.random.poisson(flux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jDYd3sK6yO9s"
   },
   "source": [
    "So we have the wavelength on the x-axis, which is assumed to have no uncertainty. The measured flux is different from the \"true\" flux due to Poisson noise. Let's plot the true flux and observed flux to see how things look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "EkfLCDwoyO9t",
    "outputId": "41ec1c73-813f-4590-99aa-89af40c2cb74"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig,ax = plt.subplots(1)\n",
    "ax.plot(wave, flux, 'r-')\n",
    "ax.step(wave, obs_flux, color='k')\n",
    "ax.set_xlabel('Wavelength Chanel')\n",
    "ax.set_ylabel('Counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IrHu3Me3yO9x"
   },
   "source": [
    "## 2 Fitting with Non-Linear Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJgZvyzeyO9y"
   },
   "source": [
    "Just to see how you can get a quick fit, let's use the non-linear least-quares (NNLS) routine scipy.optimize.curve_fit. The name \"non-linear least squares\" refers to the fact that `curve_fit` can fit models that are not linear in their parameters, so very versatile. Generally speaking, NNLS works by exploring paramters space, looking for the set of paramters that minimize the difference between the observations and the model. So we must first write a python function that defines the model we are going to fit to the data. The first argument is the x-data, the rest are the parameters to be solved (the order of the parameters will define the order of the parameter vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2RiM92RFyO9z"
   },
   "outputs": [],
   "source": [
    "def model(x, cont, slope, amp, center, width):\n",
    "    model = amp*np.exp(-0.5*np.power(x-center,2)/width**2) + cont + slope*x\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ggoxDRxdyO91"
   },
   "source": [
    "Now we run `curve_fit`, passing in the model function as the first argument, then the x and y data, an initial guess for the parameters, and the error in the observations. Since the flux has Poisson noise, we can simply put in $\\sigma(y) = \\sqrt y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "N86DQNkhyO92",
    "outputId": "ac9051bf-f9ef-47de-ba11-9a30882a0626"
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "popt,pcov = curve_fit(model, wave, obs_flux, p0=(425.,0.0,80.,4.5,1.0), sigma=np.sqrt(obs_flux))\n",
    "print(popt)\n",
    "err = np.sqrt(np.diag(pcov))\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hV45nF5cyO95"
   },
   "source": [
    "The popt variable holds the best fit parameters as a length-4 array and pcov is the 4X4 covariance matrix. The diagonal of this is the variance of each parameter, so the square root of the diagonal gives the formal errors. Let's plot out this least-squares answer and compare with the \"true\" value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "z-_95heIyO98",
    "outputId": "a7a436d7-afd0-45e0-951c-42aae28b7a19"
   },
   "outputs": [],
   "source": [
    "ax.plot(wave, model(wave, *popt), 'b-')  # Note:  *popt is a python parameter substitution trick\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2GCZPD0ByO-A"
   },
   "source": [
    "## 3 Visualizing the Covariance Matrix\n",
    "\n",
    "Aside from the best-fit values and their uncertainties, it's also a good idea to examine the covariance matrix, to see how correlated the parameters are. A quick way to do this is to construct the correlation matrix from the covariance matrix $C[i,j]$ and errors $\\sigma[i]$:\n",
    "$$\\rho[i,j] = \\frac{C[i,j]}{\\sigma[i]\\sigma[j]}$$\n",
    "positive values denote correlation, negative denote anti-correlation. $\\rho$ ranges from -1 to 1. A value close to 0 denotes no significant correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "LNfn1zjAyO-B",
    "outputId": "ea6868c9-835a-46bc-b242-2368aa8693e5"
   },
   "outputs": [],
   "source": [
    "pcor = pcov/np.outer(err,err)     # outer() does outer-multiplication\n",
    "for i in range(pcor.shape[0]):\n",
    "  for j in range(pcor.shape[1]):\n",
    "    print(\"{:5.2f} \".format(pcor[i,j]), end='')\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FCjRUU1iyO-E"
   },
   "source": [
    "From this correlation matrix, you can probably see that the continuum zero-point (first row/column) is significantly anti-correlated with the continuum slope (second row/column) and the amplitude (third row/column) is anti-correlated with the width (5th row/column). The center of the line (fourth row/column) is not significantly correlated with any of the parameters. If you think aobut it, this makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MKUN6dalyO-E"
   },
   "source": [
    "A way to visualize the correlations is to plot equal-probability ellipses in parameter space. There's no automatic way to do this that I'm aware of, so we'll follow [this procedure](https://www.visiondummy.com/2014/04/draw-error-ellipse-representing-covariance-matrix/#:~:text=The%20error%20ellipse%20represents%20an,visualize%20a%202D%20confidence%20interval.&text=This%20confidence%20ellipse%20defines%20the,from%20the%20underlying%20Gaussian%20distribution). Briefly, we'll compute  the eigenvectors and eigenvalues of the covariance matrix which gives us the major and minor axes of the ellipse. We then need to scale the whole ellipse by a factor that depends on the number of parameters we're fitting (degrees of freedom) and there are lookup tables for that, but I've just supplied the value.\n",
    "\n",
    "Matplotlib does not (yet) have a simple function to plot ellipses. We have to use the deeper-down API to first create an ellipse *artist* and then add this artist to the current axis (which we get with the <tt>gca()</tt> function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "id": "GoL_PWAWyO-F",
    "outputId": "63d51b70-26f4-4fca-97ce-17cf39ae2c82"
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "#from matplotlib.pyplot import gca,show,xlim,ylim,legend,axvline,axhline\n",
    "eigval,eigvec = np.linalg.eigh(pcov[0:2,0:2])\n",
    "if eigval[0]<eigval[1]:   # make sure eigvals are reverse-sorted\n",
    "    eigval = eigval[::-1]\n",
    "    eigvec = eigvec[:,::-1]\n",
    "# eigvec is 2X2 matrix, each eigenvector is a column. Compute angle of \n",
    "# first vector which will be the major axis of the ellipse\n",
    "theta = 180.0/np.pi*np.arctan2(eigvec[1,0],eigvec[0,0])\n",
    "# The full width and height of the 68% error ellipse is \n",
    "# 2*sqrt(eigval)*sqrt(s), where for 5 degrees of freedom, s = 5.9\n",
    "width,height = 2*np.sqrt(eigval)*np.sqrt(5.9)\n",
    "fig,ax = plt.subplots(1)\n",
    "ax.plot([popt[0]],[popt[1]], \"*\", ms=18, label=\"Best-fit solution\")\n",
    "ell = Ellipse(xy=[popt[0],popt[1]], width=width, height=height, angle=theta,\n",
    "              fc='None', ec='red')\n",
    "ax.add_artist(ell)\n",
    "# Show the real answer:\n",
    "ax.axhline(cont_slope, linestyle='--', label=\"True answer\")\n",
    "ax.axvline(cont_zp, linestyle='--')\n",
    "ax.set_xlabel('cont_zp')\n",
    "ax.set_ylabel('cont_slope')\n",
    "# Set some reasonable limits\n",
    "ax.set_xlim(popt[0]-4*err[0],popt[0]+4*err[0])\n",
    "ax.set_ylim(popt[1]-4*err[1],popt[1]+4*err[1])\n",
    "ax.legend(numpoints=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iDDEfUdFyO-I"
   },
   "source": [
    "Does the true value end up in your error ellipse? Should it? Well, if it really is a 68% error ellipse, then we would expect the true answer to end up within the ellipse 68% of the time. So if you re-run this entire notebook 100 times, you'd expect it to lie outside the ellipse about 32 times. You re-run a few times by selecting Cell->Run All from the jupyter menu. If you make the ellipse twice as large (2-sigma), then you should only end up outside the ellipse 5 times. A 3-sigma error ellipse will only fail 1% of the time. Also, if you kept track of where the best-fit solution falls with respect to the true answer each time, it should make an elliptical pattern like the one plotted above, but centered on the true value. In the [next notebook](https://colab.research.google.com/drive/15EsEFbbLiU2NFaNrfiCTlF_i65ShDlmS?usp=sharingw) you'll see how MCMC methods give us this kind of \"try it again and again\" for free.\n",
    "\n",
    "## 3 Fitting with astropy.modeling\n",
    "\n",
    "The `astropy` package has a `modeling` module that lets you do what we did above with a different user interface. Some of the advantages:\n",
    "\n",
    " - Several \"ready-to-go\" models for common problems;\n",
    " - models are objects rather than fucntions and can be combined to make more complicated models\n",
    " - parameters can be fixed, given limits, or combined ('tied') more easily\n",
    " \n",
    " First, we'll make the model object we're interested in fitting out of two built-in ones. We need a Guassian model to represent the line and a linear model for the continuum. We create these objects with some initial values for their parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.modeling import models\n",
    "line = models.Gaussian1D(amplitude=80., mean=4.5, stddev=1.0)\n",
    "continuum = models.Linear1D(slope=0.0, intercept=425.)\n",
    "model = line + continuum\n",
    "print(model.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model objects act like functions in that they can take the x-values are arguments and return the model values with the current set of paramers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model(wave))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to make a fitter that will figure out the paramters that best fit the observed data. Here, `astropy.modeling` gives us [several](https://docs.astropy.org/en/stable/modeling/reference_api.html#id3) to choose from. We have a non-linear model, so we'll use `LevMarLSQFitter` which uses the same [algorithm](https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm) as `curve_fit`. We create a fitter object and feed it the model and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.modeling import fitting\n",
    "fitter = fitting.LevMarLSQFitter()\n",
    "fitmodel = fitter(model, wave, obs_flux)\n",
    "print(fitmodel.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the fitter returns a model instance just like the initial one we gave it, but the parameters have been updated to the best-fit values. We can plot this out to make sure it did the right thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1)\n",
    "ax.plot(wave, flux, 'r-')\n",
    "ax.step(wave, obs_flux, color='k')\n",
    "ax.set_xlabel('Wavelength Chanel')\n",
    "ax.set_ylabel('Counts')\n",
    "ax.plot(wave, fitmodel(wave), 'b-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we were in a situation where the signal-to-noise is lower, but we know for sure what the central wavelength is. We might also want to ensure that the flux of the line is positive (i.e. an emission line). We can set these contraints on the variables pretty easily. One thing we need to do is figure out the paramter names. When we combined the Gaussian and linear models into what's called a compound model, `astropy` renames the parameters so there are no ambiguities. We can print `fitmodel` to see names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fitmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so we'll want to fix `mean_0` to the value `center` (defined at the beginning) and we want to place a lower-limit on `amplitude_0`. We can even re-use the `fitmodel` as our initial guess. Model fitting can be an iterative process of holding some variables fixed and letting other vary as you \"home in\" on the best-fit. We access the parameters as member variables of `fitmodel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitmodel.mean_0.fixed = True\n",
    "fitmodel.mean_0.value = center\n",
    "fitmodel.amplitude_0.min = 0\n",
    "newmodel = fitter(fitmodel, wave, obs_flux)\n",
    "print(newmodel)\n",
    "ax.plot(wave, newmodel(wave), '-g')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might try re-running this notebook with the amplitude of the line being smaller (close to the noise level) and see how the two models (with and without fixing the center wavelength) compare."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Non-linear least squares.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
