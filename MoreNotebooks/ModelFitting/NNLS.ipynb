{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6-aboTeFyO9l"
   },
   "source": [
    "# An Introduction to Model Fitting with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "asb7v9LvyO9m"
   },
   "source": [
    "In this notebook, I assume you have used python to some degree to analyze data. I will be using numpy/scipy, the de-facto numerical workhorse in python. I will also use matplotlib to visualize the data. We're going to fit a model to some 'fake' data:  a constant continuum with a Gaussian line superimposed. The [sequel to this notebook](Emcee.ipynb) will be model fitting with Markov Chain Monte Carlo techniques (MCMC). But first, let's make the fake data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-the6P_WyO9n"
   },
   "source": [
    "## 1 Making a Fake Emission Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UsoGIVwwyO9o"
   },
   "source": [
    "The \"true\" data is some background flux of photons (a continuum from the source or background) that has a linear trend, plus a Gaussian line with some amplitude, width and center. I set these up as variables so it's easy to play around with them and see how things change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XmZC5gLHyO9p"
   },
   "outputs": [],
   "source": [
    "from numpy import *  # Deal with it\n",
    "# Start by defining some parameters. Change these if you like!\n",
    "cont_zp = 500.0   # value at 0\n",
    "cont_slope = 5.0  # change in continuum per channel \n",
    "amplitude = 150.0 # peak of the line\n",
    "width = 0.5       # Width of the line\n",
    "center = 5.0      # location of the line\n",
    "\n",
    "# Next, a grid of wavelength channels (assumed to have no uncertainty)\n",
    "wave = linspace(0,10,100)\n",
    "# The 'true' observations\n",
    "flux = amplitude*exp(-0.5*power(wave-center,2)/width**2)+ \\\n",
    "       cont_zp + + cont_slope*wave\n",
    "# The actual observations = true observations + Poisson noise\n",
    "obs_flux = random.poisson(flux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jDYd3sK6yO9s"
   },
   "source": [
    "So we have the wavelength on the x-axis, which is assumed to have no uncertainty. The measured flux is different from the \"true\" flux due to Poisson noise. Let's plot the true flux and observed flux to see how things look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "EkfLCDwoyO9t",
    "outputId": "41ec1c73-813f-4590-99aa-89af40c2cb74"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.pyplot import subplots, plot,step,xlabel,ylabel,show\n",
    "fig,ax = subplots(1)\n",
    "ax.plot(wave, flux, 'r-')\n",
    "ax.step(wave, obs_flux, color='k')\n",
    "ax.set_xlabel('Wavelength Chanel')\n",
    "ax.set_ylabel('Counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IrHu3Me3yO9x"
   },
   "source": [
    "## 2 Fitting with Non-Linear Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJgZvyzeyO9y"
   },
   "source": [
    "Just to see how you can get a quick fit, let's use the non-linear least-quares routine scipy.optimize.curve_fit. To do this, we must first write a python function that defines the model we are going to fit to the data. The first argument is the x-data, the rest are parameters (the order of the parameters will define the order of the parameter vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2RiM92RFyO9z"
   },
   "outputs": [],
   "source": [
    "def model(x, cont, slope, amp, center, width):\n",
    "    model = amp*exp(-0.5*power(x-center,2)/width**2)+cont+slope*x\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ggoxDRxdyO91"
   },
   "source": [
    "Now we run curve_fit. We pass in the model function, the x and y data, an initial guess for the parameters, and the error in the observations. Since the flux has Poisson noise, we can simply put in $\\sigma(y) = \\sqrt y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "N86DQNkhyO92",
    "outputId": "ac9051bf-f9ef-47de-ba11-9a30882a0626"
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "popt,pcov = curve_fit(model, wave, obs_flux, p0=(425.,0.0,80.,4.5,1.0))\n",
    "print(popt)\n",
    "err = sqrt(diag(pcov))\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hV45nF5cyO95"
   },
   "source": [
    "The popt variable holds the best fit parameters as a length-4 array and pcov is the 4X4 covariance matrix. The diagonal of this is the variance of each parameter, so the square root of the diagonal gives the formal errors. Let's plot out this least-squares answer and compare with the \"true\" value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "z-_95heIyO98",
    "outputId": "a7a436d7-afd0-45e0-951c-42aae28b7a19"
   },
   "outputs": [],
   "source": [
    "ax.plot(wave, model(wave, *popt), 'b-')  # Note:  *popt is a python parameter substitution trick\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2GCZPD0ByO-A"
   },
   "source": [
    "Aside from the best-fit values and their uncertainties, it's also a good idea to examine the covariance matrix, to see how correlated the parameters are. A quick way to do this is to construct the correlation matrix from the covariance matrix $C[i,j]$ and errors $\\sigma[i]$:\n",
    "$$\\rho[i,j] = \\frac{C[i,j]}{\\sigma[i]\\sigma[j]}$$\n",
    "positive values denote correlation, negative denote anti-correlation. $\\rho$ ranges from -1 to 1. A value close to 0 denotes no significant correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "LNfn1zjAyO-B",
    "outputId": "ea6868c9-835a-46bc-b242-2368aa8693e5"
   },
   "outputs": [],
   "source": [
    "pcor = pcov/outer(err,err)\n",
    "for i in range(pcor.shape[0]):\n",
    "  for j in range(pcor.shape[1]):\n",
    "    print(\"{:5.2f} \".format(pcor[i,j]), end='')\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FCjRUU1iyO-E"
   },
   "source": [
    "From this correlation matrix, you can probably see that the continuum zero-point (first row/column) is significantly anti-correlated with the continuum slope (second row/column) and the amplitude (third row/column) is anti-correlated with the width (5th row/column). The center of the line (fourth row/column) is not significantly correlated with any of the parameters. If you think aobut it, this makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MKUN6dalyO-E"
   },
   "source": [
    "A way to visualize the correlations is to plot equal-probability ellipses in parameter space. There's no automatic way to do this that I'm aware of, so we'll follow [this procedure](https://www.visiondummy.com/2014/04/draw-error-ellipse-representing-covariance-matrix/#:~:text=The%20error%20ellipse%20represents%20an,visualize%20a%202D%20confidence%20interval.&text=This%20confidence%20ellipse%20defines%20the,from%20the%20underlying%20Gaussian%20distribution). Briefly, we'll compute  the eigenvectors and eigenvalues of the covariance matrix which gives us the major and minor axes of the ellipse. We then need to scale the whole ellipse by a factor that depends on the number of parameters we're fitting (degrees of freedom) and there are lookup tables for that, but I've just supplied the value.\n",
    "\n",
    "Matplotlib does not (yet) have a simple function to plot ellipses. We have to use the deeper-down API to first create an ellipse *artist* and then add this artist to the current axis (which we get with the <tt>gca()</tt> function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "id": "GoL_PWAWyO-F",
    "outputId": "63d51b70-26f4-4fca-97ce-17cf39ae2c82"
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "from matplotlib.pyplot import gca,show,xlim,ylim,legend,axvline,axhline\n",
    "eigval,eigvec = linalg.eigh(pcov[0:2,0:2])\n",
    "if eigval[0]<eigval[1]:   # make sure eigvals are reverse-sorted\n",
    "    eigval = eigval[::-1]\n",
    "    eigvec = eigvec[:,::-1]\n",
    "# eigvec is 2X2 matrix, each eigenvector is a column. Compute angle of \n",
    "# first vector which will be the major axis of the ellipse\n",
    "theta = 180.0/pi*arctan2(eigvec[1,0],eigvec[0,0])\n",
    "# The full width and height of the 68% error ellipse is \n",
    "# 2*sqrt(eigval)*sqrt(s), where for 5 degrees of freedom, s = 5.9\n",
    "width,height = 2*sqrt(eigval)*sqrt(5.9)\n",
    "plot([popt[0]],[popt[1]], \"*\", ms=18, label=\"Best-fit solution\")\n",
    "ell = Ellipse(xy=[popt[0],popt[1]], width=width, height=height, angle=theta,\n",
    "              fc='None', ec='red')\n",
    "ax = gca()\n",
    "ax.add_artist(ell)\n",
    "# Show the real answer:\n",
    "axhline(cont_slope, linestyle='--', label=\"True answer\")\n",
    "axvline(cont_zp, linestyle='--')\n",
    "xlabel('cont_zp')\n",
    "ylabel('cont_slope')\n",
    "# Set some reasonable limits\n",
    "xlim(popt[0]-4*err[0],popt[0]+4*err[0])\n",
    "ylim(popt[1]-4*err[1],popt[1]+4*err[1])\n",
    "legend(numpoints=1)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iDDEfUdFyO-I"
   },
   "source": [
    "Does the true value end up in your error ellipse? Should it? Well, if it really is a 68% error ellipse, then we would expect the true answer to end up within the ellipse 68% of the time. So if you re-run this entire notebook 100 times, you'd expect it to lie outside the ellipse about 32 times. If you make the ellipse twice as large (2-sigma), then you should only end up outside the ellipse 5 times. A 3-sigma error ellipse will only fail 1% of the time. Also, if you kept track of where the best-fit solution falls with respect to the true answer each time, it should make an elliptical pattern like the one plotted above, but centered on the true value. In the [next notebook](https://colab.research.google.com/drive/15EsEFbbLiU2NFaNrfiCTlF_i65ShDlmS?usp=sharingw) you'll see how MCMC methods give us this kind of \"try it again and again\" for free.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Non-linear least squares.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
